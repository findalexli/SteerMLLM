{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-27 05:05:03,152] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using fast tokenizer\n",
      "Using fast tokenizer\n",
      "loading base model /home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:49<00:00, 36.36s/it]\n",
      "Some weights of the model checkpoint at /home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapters from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rollout stats:\n",
      "\trollout_batch_size: 512\n",
      "\trollout_per_device_batch_size: 32\n",
      "\tworld_size: 1\n",
      "\n",
      "Step stats:\n",
      "\tstep_batch_size: 256\n",
      "\tstep_per_device_batch_size: 2\n",
      "\tworld_size: 1\n",
      "\n",
      "Accumulation steps:\n",
      "\trollout_accumulation_steps: 16\n",
      "\tgradient_accumulation_steps: 128\n",
      "\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-226d9145801c0b24/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.19s/it]\n",
      "100%|██████████| 72056/72056 [02:06<00:00, 568.64it/s]\n",
      "/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/data_utils/data_utils_ppo.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(query, dtype=torch.long).view(-1)[:-3] for query in queries\n",
      "Max query length: 554\n",
      "Filtered out 0 instances out of 72056 that exceed length limit. These examples are not used for training, but will still be used in evaluation. \n",
      "100%|██████████| 72056/72056 [01:45<00:00, 685.06it/s]\n",
      "/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/data_utils/data_utils_ppo.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(response, dtype=torch.long).view(-1)[:-3] for response in responses\n",
      "Max query length: 535\n",
      "Filtered out 0 instances out of 72056 that exceed length limit. These examples are not used for training, but will still be used in evaluation. \n"
     ]
    }
   ],
   "source": [
    "# load the rm model with the lora adapter applied to the sft model\n",
    "# make inference on a dataset of choice and get the attributesi  \n",
    "# swap the attributes back to the prompt, and \n",
    "# make inference on the same dataset for generating the output to train a attribute-conditioned model\n",
    "import json\n",
    "import gc\n",
    "import glob\n",
    "from itertools import chain\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "import datasets\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# your PyTorch code here\n",
    "\n",
    "# clean up the cache\n",
    "torch.cuda.empty_cache()\n",
    "import pdb\n",
    "import json\n",
    "import gc\n",
    "import glob\n",
    "from itertools import chain\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "from peft.utils import WEIGHTS_NAME, get_peft_model_state_dict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "from data_utils.data_utils_ppo import QueryResponseDataset\n",
    "\n",
    "import data_utils.common_utils as common_utils\n",
    "\n",
    "from data_utils.constants import AnswerType, FACTUAL_PROMPT\n",
    "\n",
    "import models.rl_models as rl_models\n",
    "\n",
    "from models.qlora_model import load_4bit_model_for_inference\n",
    "from models.reward_model import load_4bit_reward_model_for_inference\n",
    "from models.rl_trainer import (\n",
    "    AlpacaAccelerator,\n",
    "    RLTrainer,\n",
    "    remove_image_token,\n",
    "    truncate_after_eos_with_padding,\n",
    ")\n",
    "from models.ppo_trainer import remove_pad_and_left_pad\n",
    "\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "import data_utils.common_utils as utils\n",
    "logger = logging.getLogger(__name__)\n",
    "from llava import conversation as conversation_lib\n",
    "\n",
    "from peft.utils import WEIGHTS_NAME, get_peft_model_state_dict\n",
    "try:\n",
    "    from transformers import LlamaTokenizerFast as LlamaTokenizer\n",
    "\n",
    "    print(\"Using fast tokenizer\")\n",
    "except:\n",
    "    from transformers import LlamaTokenizer\n",
    "\n",
    "    print(\"Using slow tokenizer\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "from data_utils.data_utils_ppo import QueryResponseDataset\n",
    "\n",
    "import data_utils.common_utils as common_utils\n",
    "\n",
    "from data_utils.constants import AnswerType, FACTUAL_PROMPT\n",
    "\n",
    "import models.rl_models as rl_models\n",
    "\n",
    "from models.qlora_model import load_4bit_model_for_inference\n",
    "from models.reward_model import load_4bit_reward_model_for_inference\n",
    "from models.rl_trainer import (\n",
    "    AlpacaAccelerator,\n",
    "    RLTrainer,\n",
    "    remove_image_token,\n",
    "    truncate_after_eos_with_padding,\n",
    ")\n",
    "from finetune_lora_ppo import ModelArguments, TrainingArguments, DataArguments\n",
    "from data_utils.data_utils_ppo import QueryResponseDataset, QueryCoupledResponseDataset\n",
    "from data_utils.data_utils_ppo import DataCollatorForQueryResponseDataset\n",
    "class DisableLogger:\n",
    "    def __enter__(self):\n",
    "        logging.disable(logging.CRITICAL)\n",
    "\n",
    "    def __exit__(self, exit_type, exit_value, exit_traceback):\n",
    "        logging.disable(logging.NOTSET)\n",
    "\n",
    "rm_path = '/home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/rm_lora_adapter_model'\n",
    "rm_model = load_4bit_reward_model_for_inference(rm_path)\n",
    "\n",
    "vision_tower = rm_model.backbone_model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()\n",
    "vision_tower.to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "vision_tower.requires_grad_(False)\n",
    "# Tokenizer\n",
    "tokenizer_model_name = \"/home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model\"\n",
    "TokenizerClass = AutoTokenizer\n",
    "\n",
    "tokenizer = TokenizerClass.from_pretrained(\n",
    "    tokenizer_model_name,\n",
    "    # cache_dir=args.cache_dir,\n",
    "    model_max_length=2048, # taken from train_rl_model.sh\n",
    "    padding_side=\"left\",\n",
    "    truncation_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "# Dataset loading\n",
    "\n",
    "\n",
    "\n",
    "model_args = ModelArguments()\n",
    "training_args = TrainingArguments()\n",
    "data_args = DataArguments()\n",
    "data_args.dataset_path = '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_ppo50k-aokvqa12k-vqa10k.json'\n",
    "train_instructions = datasets.load_dataset(\n",
    "            \"json\", data_files=data_args.dataset_path\n",
    "        )\n",
    "train_df = pd.DataFrame(train_instructions['train'])\n",
    "# Override the base model name model_args.base_model_name\n",
    "\n",
    "model_args.base_model_name = tokenizer_model_name\n",
    "\n",
    "model_args.vision_tower = \"openai/clip-vit-large-patch14-336\"\n",
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    from llava.model import LlavaLlamaForCausalLM\n",
    "\n",
    "    with DisableLogger():\n",
    "        base_model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.base_model_name,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "        )\n",
    "\n",
    "    vision_tower = base_model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model()\n",
    "\n",
    "    data_args.image_processor = vision_tower.image_processor\n",
    "    del base_model\n",
    "\n",
    "if model_args.reward_base_model_name is None:\n",
    "    model_args.reward_base_model_name = model_args.base_model_name\n",
    "    data_args.reward_image_processor = vision_tower.image_processor\n",
    "else:\n",
    "    with DisableLogger():\n",
    "        reward_base_model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.reward_base_model_name,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "        )\n",
    "    reward_vision_tower = reward_base_model.get_vision_tower()\n",
    "    if not reward_vision_tower.is_loaded:\n",
    "        reward_vision_tower.load_model()\n",
    "    data_args.reward_image_processor = reward_vision_tower.image_processor\n",
    "    del reward_base_model\n",
    "\n",
    "data_args.is_multimodal = True\n",
    "data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "model_args.mm_vision_select_layer = -2\n",
    "\n",
    "### A couple critical parameter set borrowed fromt the launch script\n",
    "data_args.image_aspect_ratio = 'pad'\n",
    "data_args.image_folder = '/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017'\n",
    "training_args.query_len = 1280\n",
    "training_args.response_len = 768\n",
    "data_args.mm_use_im_start_end = False\n",
    "\n",
    "############## end of arguments loading\n",
    "\n",
    "if model_args.version in conversation_lib.conv_templates:\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[\n",
    "        model_args.version\n",
    "    ]\n",
    "else:\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[\n",
    "        \"vicuna_v1\"\n",
    "    ]\n",
    "train_dataset = QueryCoupledResponseDataset(\n",
    "    df=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    query_len=training_args.query_len,\n",
    "    response_len=training_args.response_len,\n",
    "    data_args=data_args,\n",
    ")\n",
    "train_dataset.query_attn_masks = train_dataset.queries.ne(tokenizer.pad_token_id).long()\n",
    "\n",
    "make_rl_data_module_output_dict = dict(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        data_collator=DataCollatorForQueryResponseDataset(),\n",
    "    )\n",
    "\n",
    "### Definition the inference class\n",
    "\n",
    "class GetAttributeModel:\n",
    "    def __init__(self, rm_model, tokenizer, accelerator: AlpacaAccelerator, training_args) -> None:\n",
    "        self.reward_model = rm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accelerator = accelerator\n",
    "        self.training_args = training_args\n",
    "        self.args = training_args # TODO add other args\n",
    "        self.args.reward_prompt_file = \"/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/scripts/13b-v1.5-336/train_reward_model.sh\"\n",
    "        self.train_dataset = make_rl_data_module_output_dict['train_dataset']\n",
    "        self.data_collator = make_rl_data_module_output_dict['data_collator']\n",
    "\n",
    "\n",
    "        self.reward_model_prompt = None\n",
    "        self.reward_model_prompt_untokenized = None\n",
    "\n",
    "        if self.args.reward_prompt_file is not None:\n",
    "            with open(self.args.reward_prompt_file, \"r\") as f:\n",
    "                self.reward_model_prompt_untokenized = \" \" + f.read().strip()\n",
    "            self.reward_model_prompt = self.tokenizer.encode(\n",
    "                self.reward_model_prompt_untokenized,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "        self.image_to_caption_mapping = None\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        logger.warning(\n",
    "            f\"Train dataset size: {len(self.train_dataset)}\",\n",
    "            # main_process_only=True\n",
    "        )  # noqa\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            collate_fn=self.data_collator,\n",
    "            batch_size=self.training_args.rollout_per_device_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        train_dataloader = self.accelerator.prepare(train_dataloader)  # noqa\n",
    "        # self._log_batch_size(train_dataloader, \"train_dataloader\")\n",
    "        return utils.InfiniteLoader(train_dataloader)\n",
    "\n",
    "    def step(self, train_dataloader, step_idx: int):\n",
    "        # TODO fix the range\n",
    "        queries_batches = [\n",
    "            next(train_dataloader) for _ in range(1)\n",
    "        ]\n",
    "        results = []\n",
    "        rollouts = self.rollout(queries_batches)\n",
    "        return rollouts\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def rollout(self, queries_data) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"Rollout trajectories with policy.\n",
    "\n",
    "        Args:\n",
    "            queries_data: Sequence of batches or DataLoader.\n",
    "                Each batch is a dict with keys 'queries' and 'query_attn_masks' and 'response'.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys\n",
    "                'queries', 'query_attn_masks', 'responses',\n",
    "                'logprobs', 'ref_logprobs', 'values',\n",
    "                'rewards', 'non_score_rewards', 'shaped_rewards'.\n",
    "        \"\"\"\n",
    "        # Give up dropout throughout.\n",
    "        # self.policy.eval()\n",
    "        # `keep_fp32_wrapper` retains the autocast wrapper of model.forward created by accelerate:\n",
    "        #  recall one sets mixed precision options with accelerator.\n",
    "        # The precise value of this arg doesn't matter here, since we use the unwrapped model only for respond.\n",
    "        # Generally, try to use the wrapped model as much as you can, since it's got the autocast/cast-back wrappers.\n",
    "\n",
    "\n",
    "        self.reward_model.eval()\n",
    "\n",
    "        rollouts = []\n",
    "        for batch_idx, batch in tqdm.tqdm(\n",
    "            enumerate(queries_data),\n",
    "            total=len(queries_data),\n",
    "            disable=not self.accelerator.is_main_process,\n",
    "            desc=\"rollout\",\n",
    "        ):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # Sample rollouts.\n",
    "            (\n",
    "                images,\n",
    "                reward_images,\n",
    "                image_file_ids,\n",
    "                caption_types,\n",
    "                length_bonus_multiplier,\n",
    "                queries,\n",
    "                query_attn_masks,\n",
    "                responses,\n",
    "            ) = common_utils.unpack_dict(\n",
    "                common_utils.prepare_inputs(batch, device=self.accelerator.device),\n",
    "                keys=(\n",
    "                    \"images\",\n",
    "                    \"reward_images\",\n",
    "                    \"image_file_ids\",\n",
    "                    \"caption_types\",\n",
    "                    \"length_bonus_multiplier\",\n",
    "                    \"queries\",\n",
    "                    \"query_attn_masks\",\n",
    "                    \"responses\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            if self.args.bf16:\n",
    "                images = images.to(torch.bfloat16)\n",
    "                reward_images = reward_images.to(torch.bfloat16)\n",
    "            elif self.args.fp16:\n",
    "                images = images.half()\n",
    "                reward_images = reward_images.half()\n",
    "            # TODO: replace with the response from the dataset, not model generated\n",
    "            # respond_outputs = unwrapped_policy.respond(\n",
    "            #     queries, query_attn_masks, images, temperature=self.args.temperature\n",
    "            # )\n",
    "            # (responses,) = common_utils.unpack_dict(respond_outputs, (\"responses\",))\n",
    "\n",
    "            additional_token1 = self.tokenizer.encode(\"?\", add_special_tokens=False)[0]\n",
    "            assert additional_token1 == 1577\n",
    "\n",
    "            additional_token2 = self.tokenizer.encode(\"\\n?\")[-1]\n",
    "            assert additional_token2 == 29973\n",
    "\n",
    "            responses = truncate_after_eos_with_padding(\n",
    "                responses,\n",
    "                self.tokenizer.eos_token_id,\n",
    "                self.tokenizer.pad_token_id,\n",
    "                additional_tokens=[additional_token1, additional_token2],\n",
    "            )\n",
    "\n",
    "            rollouts_batch = {\n",
    "                \"images\": images,\n",
    "                \"reward_images\": reward_images,\n",
    "                \"queries\": queries,\n",
    "                \"query_attn_masks\": query_attn_masks,\n",
    "                \"responses\": responses,\n",
    "            }\n",
    "            # Decode the response \n",
    "            text_responses = self.tokenizer.batch_decode(\n",
    "                responses,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "            correct_bonus = []\n",
    "            # go through the text responses and assign bonus (optional)\n",
    "            # for idx, response in enumerate(text_responses):\n",
    "            #     caption_type = AnswerType(caption_types[idx].item())\n",
    "\n",
    "            #     if caption_type == AnswerType.GENERAL:\n",
    "            #         correct_bonus.append(0.0)\n",
    "            #     elif caption_type in [AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD]:\n",
    "            #         expected_start = caption_type.name.split(\"_\")[0] + \".\"\n",
    "            #         expected_phrase = \"correct option is \" + expected_start\n",
    "            #         if response.strip().startswith(expected_start) or expected_phrase in response:\n",
    "            #             correct_bonus.append(1.0)\n",
    "            #         else:\n",
    "            #             correct_bonus.append(0.0)\n",
    "            #     elif caption_type == AnswerType.NO_IN_YESNO:\n",
    "            #         if response.strip().startswith(\"No\"):\n",
    "            #             correct_bonus.append(0.5)\n",
    "            #         elif response.strip().startswith(\"Yes\"):\n",
    "            #             correct_bonus.append(-0.5)\n",
    "            #         else:\n",
    "            #             correct_bonus.append(0.0)\n",
    "            #     elif caption_type == AnswerType.YES_IN_YESNO:\n",
    "            #         # TODO(zhiqings): for now, we do not give symbolic award for \"Yes\" in Yes/No questions.\n",
    "            #         correct_bonus.append(0.0)\n",
    "            #     else:\n",
    "            #         raise NotImplementedError\n",
    "            # assert len(correct_bonus) == len(text_responses)\n",
    "            # correct_bonus = torch.tensor(correct_bonus, device=responses.device)\n",
    "\n",
    "            has_stop_token = [\n",
    "                self.tokenizer.eos_token_id in response\n",
    "                for response in responses.tolist()\n",
    "            ]\n",
    "\n",
    "            sequences = [\n",
    "                torch.concat((query, response), dim=0)\n",
    "                for query, response in zip(queries, responses)\n",
    "            ]\n",
    "            sequences = torch.stack(sequences, dim=0)\n",
    "\n",
    "            sequences = remove_pad_and_left_pad(\n",
    "                sequences,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "            # Prepareing the reward model prompt, \n",
    "            # may have fancy way to add captions or context to the prompt, dubbed as FACTUAL-RLHF in LLaVa-RLHF paper\n",
    "            if self.reward_model_prompt is not None:\n",
    "                if self.image_to_caption_mapping is None:\n",
    "                    reward_model_prompt = (\n",
    "                        self.reward_model_prompt.reshape(1, -1)\n",
    "                        .repeat(len(sequences), 1)\n",
    "                        .to(self.accelerator.device)\n",
    "                    )\n",
    "                    sequences = torch.cat((sequences, reward_model_prompt), dim=1)\n",
    "                else:\n",
    "                    reward_model_prompt_untokenized = (\n",
    "                        self.reward_model_prompt_untokenized\n",
    "                    )\n",
    "                    image_to_caption_mapping = self.image_to_caption_mapping\n",
    "\n",
    "                    image_ids = []\n",
    "                    for i in range(len(sequences)):\n",
    "                        image_file = str(image_file_ids[i].item()).zfill(12) + \".jpg\"\n",
    "                        caption_type = AnswerType(caption_types[i].item())\n",
    "                        if caption_type in [AnswerType.GENERAL, AnswerType.NO_IN_YESNO, AnswerType.YES_IN_YESNO]:\n",
    "                            image_id = image_file\n",
    "                        elif caption_type in [AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD]:\n",
    "                            image_id = \"aok_\" + image_file\n",
    "                        else:\n",
    "                            print(caption_type)\n",
    "                            print([AnswerType.GENERAL, AnswerType.NO_IN_YESNO, AnswerType.YES_IN_YESNO])\n",
    "                            print([AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD])\n",
    "                            raise NotImplementedError\n",
    "                        image_ids.append(image_id)\n",
    "\n",
    "                    captions = [\n",
    "                        image_to_caption_mapping[image_id] for image_id in image_ids\n",
    "                    ]\n",
    "\n",
    "                    assert r\"{factual_prompt}\" in reward_model_prompt_untokenized\n",
    "\n",
    "                    reward_model_prompts = []\n",
    "\n",
    "                    for caption_list in captions:\n",
    "                        caption_list = caption_list[:]\n",
    "                        random.shuffle(caption_list)\n",
    "                        factual_prompt = FACTUAL_PROMPT\n",
    "                        for caption in caption_list:\n",
    "                            factual_prompt = factual_prompt + f\"  - {caption}\\n\"\n",
    "                        reward_model_prompt_per_example = (\n",
    "                            reward_model_prompt_untokenized.format(\n",
    "                                factual_prompt=factual_prompt\n",
    "                            )\n",
    "                        )\n",
    "                        reward_model_prompts.append(reward_model_prompt_per_example)\n",
    "                    reward_model_prompts = self.tokenizer(\n",
    "                        reward_model_prompts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        add_special_tokens=False,\n",
    "                        padding=\"longest\",\n",
    "                    )[\"input_ids\"]\n",
    "                    reward_model_prompts = reward_model_prompts.to(\n",
    "                        self.accelerator.device\n",
    "                    )\n",
    "\n",
    "                    sequences = torch.cat((sequences, reward_model_prompts), dim=1)\n",
    "                    sequences = remove_pad_and_left_pad(\n",
    "                        sequences,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    )\n",
    "\n",
    "            clean_sequences = sequences.detach().clone()\n",
    "            clean_sequences[\n",
    "                clean_sequences == IMAGE_TOKEN_INDEX\n",
    "            ] = self.tokenizer.eos_token_id\n",
    "\n",
    "            text_sequences = self.tokenizer.batch_decode(\n",
    "                clean_sequences,\n",
    "                skip_special_tokens=False,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "            if self.accelerator.is_main_process:\n",
    "                print(\"=\" * 20)\n",
    "                print(text_sequences[0].split(\"<unk><s> \")[-1])\n",
    "                print(\"=\" * 20)\n",
    "                image_id = image_file_ids[0].item()\n",
    "                # convert int into \"000000xxxxxx.jpg\"\n",
    "                image_id = (\n",
    "                    \"https://s3.us-east-1.amazonaws.com/images.cocodataset.org/train2017/\"\n",
    "                    + str(image_id).zfill(12)\n",
    "                    + \".jpg\"\n",
    "                )\n",
    "                print(image_id)\n",
    "                print(\"=\" * 20)\n",
    "            # OPTIONAL: compute the length bonus \n",
    "            non_pad_mask = responses.ne(self.tokenizer.pad_token_id)\n",
    "            non_pad_seq_len = (\n",
    "                non_pad_mask.sum(dim=1).float().to(self.accelerator.device)\n",
    "            )\n",
    "            length_bonus = non_pad_seq_len / float(self.args.response_len)\n",
    "\n",
    "            # convert length_bonus_multiplier to the shape, type, and device of length_bonus\n",
    "            length_bonus = length_bonus * length_bonus_multiplier.to(\n",
    "                length_bonus.device\n",
    "            ).reshape(length_bonus.shape).to(length_bonus.dtype)\n",
    "\n",
    "            sequences_attention_mask = sequences.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "            # Evaluate logprobs of the samples.\n",
    "\n",
    "\n",
    "\n",
    "            rollouts_batch[\"length_bonus\"] = length_bonus\n",
    "            rollouts_batch[\"correct_bonus\"] = correct_bonus\n",
    "            sub_batch_size = self.args.reward_model_per_device_batch_size\n",
    "            batch_size_per_device = rollouts_batch[\"responses\"].shape[0]\n",
    "            if sub_batch_size is None or sub_batch_size == batch_size_per_device:\n",
    "                reward_outputs = self.reward_model(\n",
    "                    input_ids=sequences,\n",
    "                    attention_mask=sequences_attention_mask,\n",
    "                    images=reward_images,\n",
    "                )\n",
    "                print(reward_outputs)\n",
    "            else:\n",
    "                assert batch_size_per_device % sub_batch_size == 0\n",
    "\n",
    "                reward_outputs_list = []\n",
    "\n",
    "                for sub_batch_idx in range(batch_size_per_device // sub_batch_size):\n",
    "                    idx_start = sub_batch_idx * sub_batch_size\n",
    "                    idx_end = (sub_batch_idx + 1) * sub_batch_size\n",
    "                    sub_batch_reward_outputs = self.reward_model(\n",
    "                        input_ids=sequences[idx_start:idx_end],\n",
    "                        attention_mask=sequences_attention_mask[idx_start:idx_end],\n",
    "                        images=reward_images[idx_start:idx_end],\n",
    "                    )\n",
    "                    reward_outputs_list.append(sub_batch_reward_outputs)\n",
    "\n",
    "                reward_outputs = common_utils.merge_dict(\n",
    "                    reward_outputs_list, merge_fn=torch.cat\n",
    "                )\n",
    "                del reward_outputs_list\n",
    "                del sub_batch_reward_outputs\n",
    "            # Remove the penality for sequences that did not stop properly\n",
    "            # reward_outputs = self.post_reward(\n",
    "            #     reward_outputs,\n",
    "            #     responses,\n",
    "            #     penalize_no_stop_token=self.args.penalize_no_stop_token,\n",
    "            #     relative_stop_token_penalty=self.args.relative_stop_token_penalty,\n",
    "            #     has_stop_token=has_stop_token,\n",
    "            # )\n",
    "            rollouts_batch.update(reward_outputs)\n",
    "            print(f'rollouts_batch: {rollouts_batch}')\n",
    "\n",
    "            # Shape reward with KL penalty.\n",
    "            # shape_reward_outputs = self._shape_reward(\n",
    "            #     rewards=rollouts_batch[\"rewards\"],\n",
    "            #     responses=rollouts_batch[\"responses\"],\n",
    "            #     logprobs=rollouts_batch[\"logprobs\"],\n",
    "            #     ref_logprobs=rollouts_batch[\"ref_logprobs\"],\n",
    "            #     length_bonus=rollouts_batch[\"length_bonus\"],\n",
    "            #     correct_bonus=rollouts_batch[\"correct_bonus\"],\n",
    "            # )\n",
    "            rollouts_batch_cpu = {\n",
    "                key: value for key, value in rollouts_batch.items()\n",
    "            }\n",
    "            rollouts.append(rollouts_batch_cpu)\n",
    "\n",
    "        # # Items in dict need to be of same shape.\n",
    "        # rollouts = common_utils.merge_dict(rollouts, merge_fn=torch.cat)\n",
    "\n",
    "        # # Estimating advantages outside the loop gives more samples for reward normalization.\n",
    "        # advantages = self._estimate_advantage(\n",
    "        #     rewards=rollouts[\"shaped_rewards\"].to(self.accelerator.device),\n",
    "        #     values=rollouts[\"values\"].to(self.accelerator.device),\n",
    "        # )\n",
    "        # advantages = {key: value.cpu() for key, value in advantages.items()}\n",
    "        # pdb.set_trace()\n",
    "        return rollouts_batch\n",
    "    \n",
    "############ End of definition of the inference task ###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/accelerator.py:371: UserWarning: `log_with=[]` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Train dataset size: 72056\n"
     ]
    }
   ],
   "source": [
    "accelerator = AlpacaAccelerator(\n",
    "log_with=training_args.report_to,\n",
    "project_dir=training_args.logging_dir,\n",
    "gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "even_batches=True,  # Make sure the batch size on each device is the same.\n",
    "split_batches=False,  # Don't break a batch into smaller chunks.\n",
    "step_scheduler_with_optimizer=False,  # Untie optimizer and scheduler step.\n",
    "# Value model might not use all parameters (e.g., lm-head) in the forward pass.\n",
    "kwargs_handlers=[\n",
    "    DistributedDataParallelKwargs(\n",
    "        find_unused_parameters=training_args.ddp_find_unused_parameters,\n",
    "    )\n",
    "],\n",
    ")\n",
    "\n",
    "attribute_model = GetAttributeModel(rm_model, tokenizer, accelerator, training_args)\n",
    "attribute_model.training_args.rollout_per_device_batch_size = 16\n",
    "train_dataloader = attribute_model.get_train_dataloader()\n",
    "for i in range(1):\n",
    "    rollout = attribute_model.step(train_dataloader, i)\n",
    "    print(rollout.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-25 23:33:18,660] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "loading base model /home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:53<00:00, 37.71s/it]\n",
      "Some weights of the model checkpoint at /home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapters from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# load the rm model with the lora adapter applied to the sft model\n",
    "# make inference on a dataset of choice and get the attributesi  \n",
    "# swap the attributes back to the prompt, and \n",
    "# make inference on the same dataset for generating the output to train a attribute-conditioned model\n",
    "import json\n",
    "import gc\n",
    "import glob\n",
    "from itertools import chain\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "from peft.utils import WEIGHTS_NAME, get_peft_model_state_dict\n",
    "\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "from data_utils.data_utils_ppo import QueryResponseDataset\n",
    "\n",
    "import data_utils.common_utils as common_utils\n",
    "\n",
    "from data_utils.constants import AnswerType, FACTUAL_PROMPT\n",
    "\n",
    "import models.rl_models as rl_models\n",
    "\n",
    "from models.qlora_model import load_4bit_model_for_inference\n",
    "from models.reward_model import load_4bit_reward_model_for_inference\n",
    "from models.rl_trainer import (\n",
    "    AlpacaAccelerator,\n",
    "    RLTrainer,\n",
    "    remove_image_token,\n",
    "    truncate_after_eos_with_padding,\n",
    ")\n",
    "rm_path = '/home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/rm_lora_adapter_model'\n",
    "rm_model = load_4bit_reward_model_for_inference(rm_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionTower(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_tower = rm_model.backbone_model.get_vision_tower()\n",
    "if not vision_tower.is_loaded:\n",
    "    vision_tower.load_model()\n",
    "vision_tower.to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "vision_tower.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fast tokenizer\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import LlamaTokenizerFast as LlamaTokenizer\n",
    "\n",
    "    print(\"Using fast tokenizer\")\n",
    "except:\n",
    "    from transformers import LlamaTokenizer\n",
    "\n",
    "    print(\"Using slow tokenizer\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer loading\n",
    "\n",
    "tokenizer_model_name = \"/home/ubuntu/RLHF/LLaVA-RLHF-13b-v1.5-336/sft_model\"\n",
    "TokenizerClass = AutoTokenizer\n",
    "\n",
    "tokenizer = TokenizerClass.from_pretrained(\n",
    "    tokenizer_model_name,\n",
    "    # cache_dir=args.cache_dir,\n",
    "    model_max_length=2048, # taken from train_rl_model.sh\n",
    "    padding_side=\"left\",\n",
    "    truncation_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. lets see the RM model applied to the RLHF data\n",
    "\n",
    "RLHF data is made of \n",
    "\n",
    "\n",
    "\n",
    "The PPO training of the policy model is based on the prompt combination of:\n",
    "\n",
    "Our RL split of the LLaVA data (50k)\n",
    "A-OKVQA in the CoT format (12k)\n",
    "Yes/No Questions from VQA-v2 (10k)\n",
    "\n",
    "present in the LLaVA-RLHF-Data/llava_ppo50k-aokvqa12k-vqa10k.json file. \n",
    "\n",
    "\n",
    "partially implemented in the rollout method in ppo_trainer (torch inference mode def rollout)\n",
    "\n",
    "Line 181 until 550 without the KL penality stuff\n",
    "\n",
    "\n",
    "lets say we have rewards per sample, a float applied. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-226d9145801c0b24/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    dataset_path: str = field(default=\"tatsu-lab/alpaca_farm\")\n",
    "    dataset_name: str = field(default=\"alpaca_instructions\")\n",
    "    train_splits: List[str] = field(default_factory=lambda: [\"unlabeled\"])\n",
    "    stop_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Token to stop generation with.\"},\n",
    "    )\n",
    "    # From LLaVA\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = \"square\"\n",
    "    image_grid_pinpoints: Optional[str] = field(default=None)\n",
    "\n",
    "DataArguments.dataset_path = '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_ppo50k-aokvqa12k-vqa10k.json'\n",
    "train_instructions = datasets.load_dataset(\n",
    "            \"json\", data_files=DataArguments.dataset_path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is takenfrom data_utils_ppo and make_rl_data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_instructions['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'human',\n",
       "  'value': \"<image>\\nWhat activity could develop the young girl's physical and cognitive abilities?\"},\n",
       " {'from': 'gpt',\n",
       "  'value': \"Flying a kite, like in the image, can be a fun activity that helps develop a young girl's physical and cognitive abilities. This activity encourages physical movement, such as running in open spaces, and helps improve hand-eye coordination as the child navigates the kite in the sky. Additionally, flying a kite requires problem-solving and strategic thinking, as the child must understand wind patterns and make adjustments to maintain the kite's flight. Overall, kite flying not only serves as a recreational activity but also contributes to the child's growth and development.\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]['conversations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The difference is that in RL, the response is sampled from the policy (model) whereas here I am making inferennce given (query, response) to a score giveen by the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DisableLogger:\n",
    "    def __enter__(self):\n",
    "        logging.disable(logging.CRITICAL)\n",
    "\n",
    "    def __exit__(self, exit_type, exit_value, exit_traceback):\n",
    "        logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rollout stats:\n",
      "\trollout_batch_size: 512\n",
      "\trollout_per_device_batch_size: 32\n",
      "\tworld_size: 1\n",
      "\n",
      "Step stats:\n",
      "\tstep_batch_size: 256\n",
      "\tstep_per_device_batch_size: 2\n",
      "\tworld_size: 1\n",
      "\n",
      "Accumulation steps:\n",
      "\trollout_accumulation_steps: 16\n",
      "\tgradient_accumulation_steps: 128\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fast tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from finetune_lora_ppo import ModelArguments, TrainingArguments, DataArguments\n",
    "model_args = ModelArguments()\n",
    "training_args = TrainingArguments()\n",
    "data_args = DataArguments()\n",
    "# Override the base model name model_args.base_model_name\n",
    "\n",
    "model_args.base_model_name = tokenizer_model_name\n",
    "\n",
    "VISION_TOWER=\"openai/clip-vit-large-patch14-336\"\n",
    "model_args.vision_tower = VISION_TOWER\n",
    "\n",
    "if model_args.vision_tower is not None:\n",
    "    from llava.model import LlavaLlamaForCausalLM\n",
    "\n",
    "    with DisableLogger():\n",
    "        base_model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.base_model_name,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "        )\n",
    "\n",
    "    vision_tower = base_model.get_vision_tower()\n",
    "    if not vision_tower.is_loaded:\n",
    "        vision_tower.load_model()\n",
    "\n",
    "    data_args.image_processor = vision_tower.image_processor\n",
    "    del base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "if model_args.reward_base_model_name is None:\n",
    "    model_args.reward_base_model_name = model_args.base_model_name\n",
    "    data_args.reward_image_processor = vision_tower.image_processor\n",
    "else:\n",
    "    with DisableLogger():\n",
    "        reward_base_model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "            model_args.reward_base_model_name,\n",
    "            cache_dir=training_args.cache_dir,\n",
    "        )\n",
    "    reward_vision_tower = reward_base_model.get_vision_tower()\n",
    "    if not reward_vision_tower.is_loaded:\n",
    "        reward_vision_tower.load_model()\n",
    "    data_args.reward_image_processor = reward_vision_tower.image_processor\n",
    "    del reward_base_model\n",
    "\n",
    "data_args.is_multimodal = True\n",
    "data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "model_args.mm_vision_select_layer = -2\n",
    "\n",
    "### A couple critical parameter set borrowed fromt the launch script\n",
    "data_args.image_aspect_ratio = 'pad'\n",
    "data_args.image_folder = '/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.data_utils_ppo import QueryResponseDataset, QueryCoupledResponseDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.query_len = 1280\n",
    "training_args.response_len = 768\n",
    "data_args.mm_use_im_start_end = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72056/72056 [02:06<00:00, 569.17it/s]\n",
      "/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/data_utils/data_utils_ppo.py:267: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(query, dtype=torch.long).view(-1)[:-3] for query in queries\n",
      "Max query length: 554\n",
      "Filtered out 0 instances out of 72056 that exceed length limit. These examples are not used for training, but will still be used in evaluation. \n",
      "100%|██████████| 72056/72056 [01:46<00:00, 674.64it/s]\n",
      "/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/data_utils/data_utils_ppo.py:320: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(response, dtype=torch.long).view(-1)[:-3] for response in responses\n",
      "Max query length: 535\n",
      "Filtered out 0 instances out of 72056 that exceed length limit. These examples are not used for training, but will still be used in evaluation. \n"
     ]
    }
   ],
   "source": [
    "from data_utils.data_utils_ppo import QueryResponseDataset, QueryCoupledResponseDataset\n",
    "from llava import conversation as conversation_lib\n",
    "\n",
    "if model_args.version in conversation_lib.conv_templates:\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[\n",
    "        model_args.version\n",
    "    ]\n",
    "else:\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[\n",
    "        \"vicuna_v1\"\n",
    "    ]\n",
    "train_dataset = QueryCoupledResponseDataset(\n",
    "    df=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    query_len=training_args.query_len,\n",
    "    response_len=training_args.response_len,\n",
    "    data_args=data_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.query_attn_masks = train_dataset.queries.ne(tokenizer.pad_token_id).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "        29889,  3148,  1001, 29901, 29871,    13,   319,  1799,  9047, 13566,\n",
       "        29901,   383,  5890,   263,   413,   568, 29892,   763,   297,   278,\n",
       "         1967, 29892,   508,   367,   263,  2090,  6354,   393,  6911,  2693,\n",
       "          263,  4123,  7826, 29915, 29879,  9128,   322, 25323,  3321,   633,\n",
       "         9770, 29889,   910,  6354, 18443,   267,  9128, 10298, 29892,  1316,\n",
       "          408,  2734,   297,  1722,  8162, 29892,   322,  6911, 11157,  1361,\n",
       "        29899,  1032, 29872, 29311,  3381,   408,   278,  2278, 12402,  1078,\n",
       "          278,   413,   568,   297,   278, 14744, 29889, 19814, 29892, 22764,\n",
       "          263,   413,   568,  6858,  1108, 29899,  2929,  1747,   322, 16650,\n",
       "          293,  7291, 29892,   408,   278,  2278,  1818,  2274,  8805, 15038,\n",
       "          322,  1207, 10365,  1860,   304,  7344,   278,   413,   568, 29915,\n",
       "        29879, 16286, 29889,  6811,   497, 29892,   413,   568, 22764,   451,\n",
       "          871, 19700,   408,   263, 28709,  1288,  6354,   541,   884,   640,\n",
       "         5026,   304,   278,  2278, 29915, 29879, 14321,   322])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['responses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A couple critical parameter set borrowed fromt the launch script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.image_aspect_ratio = 'pad'\n",
    "data_args.image_folder = '/home/ubuntu/latest_llava/llava_1dot5data/coco/train2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO fix the length (query_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.data_utils_ppo import DataCollatorForQueryResponseDataset\n",
    "make_rl_data_module_output_dict = dict(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        data_collator=DataCollatorForQueryResponseDataset(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/accelerator.py:371: UserWarning: `log_with=[]` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Train dataset size: 72056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rollout:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: </s> \n",
      "What issue does the sign address, and why is it important? ASSISTANT: The sign in the image addresses the issue of hazardous road conditions, specifically warning cyclists to be cautious in the area due to potential holes or uneven surfaces that may cause them to fall off their bicycles. It is important as it serves to alert cyclists to potential dangers so they can be prepared, slow down, and take necessary precautions to ensure their safety. This indication can help prevent accidents and injuries, contributing to a safer environment for both cyclists and other road<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: \n",
      " ASSISTANT: The sign in the image addresses the issue of hazardous road conditions, specifically warning cyclists to be cautious in the area due to potential holes or uneven surfaces that may cause them to fall off their bicycles. It is important as it serves to alert cyclists to potential dangers so they can be prepared, slow down, and take necessary precautions to ensure their safety. This indication can help prevent accidents and injuries, contributing to a safer environment for both cyclists and other road  #!/bin/bash\n",
      "\n",
      "set -e\n",
      "set -x\n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "export DATA_DIR=\"/path/to/your/data/directory\"\n",
      "export MODEL_DIR=\"/path/to/your/model/directory\"\n",
      "export PYTHONPATH=\"$PWD:$PYTHONPATH\"\n",
      "export GPUS_PER_NODE=8\n",
      "export OMP_NUM_THREADS=8\n",
      "\n",
      "\n",
      "# MODEL CONFIG\n",
      "VISION_TOWER=openai/clip-vit-large-patch14-336\n",
      "LM_MODEL_NAME=LLaVA-RLHF-13b-v1.5-336/sft_model\n",
      "\n",
      "# DATA CONFIG\n",
      "PREFERENCE_DATA=llava_7b_v1_preference.json\n",
      "\n",
      "# SAVE CONFIG\n",
      "MODEL_NAME=LLaVA-Fact-RM-13b-v1.5-336-lora-padding\n",
      "\n",
      "# TRAINING CONFIG\n",
      "NUM_EPOCHS=1\n",
      "LEARNING_RATE=2e-5\n",
      "BATCH_SIZE=4\n",
      "GRAD_ACCUMULATION=1\n",
      "\n",
      "torchrun \\\n",
      "    --standalone \\\n",
      "    --nnodes=1 \\\n",
      "    --nproc-per-node=$GPUS_PER_NODE \\\n",
      "    finetune_lora_rm.py \\\n",
      "    --do_train \\\n",
      "    --do_eval \\\n",
      "    --seed 42 \\\n",
      "    --per_device_train_batch_size $BATCH_SIZE \\\n",
      "    --per_device_eval_batch_size $BATCH_SIZE \\\n",
      "    --gradient_accumulation_steps $GRAD_ACCUMULATION \\\n",
      "    --model_name_or_path $MODEL_DIR/$LM_MODEL_NAME \\\n",
      "    --image_folder $DATA_DIR/coco/train2017 \\\n",
      "    --vision_tower $VISION_TOWER \\\n",
      "    --learning_rate $LEARNING_RATE \\\n",
      "    --mm_vision_select_layer -2 \\\n",
      "    --mm_use_im_start_end False \\\n",
      "    --mm_use_im_patch_token False \\\n",
      "    --freeze_mm_mlp_adapter True \\\n",
      "    --model_max_length 2048 \\\n",
      "    --query_len 1280 \\\n",
      "    --response_len 768 \\\n",
      "    --dataset_path $DATA_DIR/$PREFERENCE_DATA \\\n",
      "    --eval_dataset_path $DATA_DIR/$PREFERENCE_DATA \\\n",
      "    --dataset_name \"none\" \\\n",
      "    --eval_dataset_name \"none\" \\\n",
      "    --eval_size 500 \\\n",
      "    --bits 16 \\\n",
      "    --lora_r 64 \\\n",
      "    --lora_modules q_proj k_proj v_proj o_proj gate_proj up_proj down_proj \\\n",
      "    --output_dir \"$MODEL_DIR/$MODEL_NAME\" \\\n",
      "    --num_train_epochs $NUM_EPOCHS \\\n",
      "    --group_by_length False \\\n",
      "    --evaluation_strategy \"steps\" \\\n",
      "    --eval_steps 50 \\\n",
      "    --save_strategy \"steps\" \\\n",
      "    --save_steps 50 \\\n",
      "    --save_total_limit 10 \\\n",
      "    --weight_decay 0.0 \\\n",
      "    --warmup_ratio 0.03 \\\n",
      "    --lr_scheduler_type \"constant_with_warmup\" \\\n",
      "    --logging_steps 5 \\\n",
      "    --report_to \"tensorboard\" \\\n",
      "    --ddp_backend \"nccl\" \\\n",
      "    --bf16 True \\\n",
      "    --ddp_find_unused_parameters False \\\n",
      "    --resume_from_training True \\\n",
      "    --reward_prompt_file \"./prompts/fact_rlhf_reward_prompt.txt\" \\\n",
      "    --image_to_caption_file \"$DATA_DIR/image_to_caption.json\" \\\n",
      "    --image_aspect_ratio 'pad'\n",
      "====================\n",
      "https://s3.us-east-1.amazonaws.com/images.cocodataset.org/train2017/000000186785.jpg\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rollout:   0%|          | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 11.71 GiB (GPU 0; 79.15 GiB total capacity; 52.41 GiB already allocated; 8.61 GiB free; 69.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=468'>469</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m attribute_model\u001b[39m.\u001b[39mget_train_dataloader()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=469'>470</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=470'>471</a>\u001b[0m     rollout \u001b[39m=\u001b[39m attribute_model\u001b[39m.\u001b[39;49mstep(train_dataloader, i)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=471'>472</a>\u001b[0m     \u001b[39mprint\u001b[39m(rollout\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=472'>473</a>\u001b[0m     \u001b[39m# Saves the attributes to the prompt\u001b[39;00m\n",
      "\u001b[1;32m/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m queries_batches \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mnext\u001b[39m(train_dataloader) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m ]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m rollouts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout(queries_batches)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39mreturn\u001b[39;00m rollouts\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=362'>363</a>\u001b[0m batch_size_per_device \u001b[39m=\u001b[39m rollouts_batch[\u001b[39m\"\u001b[39m\u001b[39mresponses\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=363'>364</a>\u001b[0m \u001b[39mif\u001b[39;00m sub_batch_size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m sub_batch_size \u001b[39m==\u001b[39m batch_size_per_device:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=364'>365</a>\u001b[0m     reward_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_model(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=365'>366</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49msequences,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=366'>367</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49msequences_attention_mask,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=367'>368</a>\u001b[0m         images\u001b[39m=\u001b[39;49mreward_images,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=368'>369</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=369'>370</a>\u001b[0m     \u001b[39mprint\u001b[39m(reward_outputs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=370'>371</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/RLHF/LLaVA-RLHF/RLHF/models/reward_model.py:193\u001b[0m, in \u001b[0;36mRewardModel.forward\u001b[0;34m(self, input_ids, attention_mask, images, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone_model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[39m# print(input_ids.shape, images.shape, 'images', images.dtype)\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone_model(\n\u001b[1;32m    194\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    195\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    196\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    198\u001b[0m     images\u001b[39m=\u001b[39;49mimages,\n\u001b[1;32m    199\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    202\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(last_hidden_state, torch\u001b[39m.\u001b[39mTensor), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/peft/peft_model.py:922\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    912\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    913\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    914\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 922\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    923\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    926\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    929\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    930\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    933\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/latest_llava/LLaVA/llava/model/language_model/llava_llama.py:78\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, return_dict)\u001b[0m\n\u001b[1;32m     75\u001b[0m input_ids, attention_mask, past_key_values, inputs_embeds, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_labels_for_multimodal(input_ids, attention_mask, past_key_values, labels, images)\n\u001b[1;32m     77\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m     79\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m     80\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     81\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m     82\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m     83\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m     84\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m     85\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m     86\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     90\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    687\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    407\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    409\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    410\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    411\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    412\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    413\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    414\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    415\u001b[0m )\n\u001b[1;32m    416\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    418\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/llavav2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:330\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    327\u001b[0m key_states \u001b[39m=\u001b[39m repeat_kv(key_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    328\u001b[0m value_states \u001b[39m=\u001b[39m repeat_kv(value_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 330\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m)) \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim)\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m attn_weights\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    333\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention weights should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\u001b[39m \u001b[39mq_len,\u001b[39m \u001b[39mkv_seq_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_weights\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.71 GiB (GPU 0; 79.15 GiB total capacity; 52.41 GiB already allocated; 8.61 GiB free; 69.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# your PyTorch code here\n",
    "\n",
    "# clean up the cache\n",
    "torch.cuda.empty_cache()\n",
    "import pdb\n",
    "import json\n",
    "import gc\n",
    "import glob\n",
    "from itertools import chain\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "from peft.utils import WEIGHTS_NAME, get_peft_model_state_dict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from llava.constants import (\n",
    "    IGNORE_INDEX,\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    ")\n",
    "\n",
    "from data_utils.data_utils_ppo import QueryResponseDataset\n",
    "\n",
    "import data_utils.common_utils as common_utils\n",
    "\n",
    "from data_utils.constants import AnswerType, FACTUAL_PROMPT\n",
    "\n",
    "import models.rl_models as rl_models\n",
    "\n",
    "from models.qlora_model import load_4bit_model_for_inference\n",
    "from models.reward_model import load_4bit_reward_model_for_inference\n",
    "from models.rl_trainer import (\n",
    "    AlpacaAccelerator,\n",
    "    RLTrainer,\n",
    "    remove_image_token,\n",
    "    truncate_after_eos_with_padding,\n",
    ")\n",
    "from models.ppo_trainer import remove_pad_and_left_pad\n",
    "\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "import data_utils.common_utils as utils\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GetAttributeModel:\n",
    "    def __init__(self, rm_model, tokenizer, accelerator: AlpacaAccelerator, training_args) -> None:\n",
    "        self.reward_model = rm_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.accelerator = accelerator\n",
    "        self.training_args = training_args\n",
    "        self.args = training_args # TODO add other args\n",
    "        self.args.reward_prompt_file = \"/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/scripts/13b-v1.5-336/train_reward_model.sh\"\n",
    "        self.train_dataset = make_rl_data_module_output_dict['train_dataset']\n",
    "        self.data_collator = make_rl_data_module_output_dict['data_collator']\n",
    "\n",
    "\n",
    "        self.reward_model_prompt = None\n",
    "        self.reward_model_prompt_untokenized = None\n",
    "\n",
    "        if self.args.reward_prompt_file is not None:\n",
    "            with open(self.args.reward_prompt_file, \"r\") as f:\n",
    "                self.reward_model_prompt_untokenized = \" \" + f.read().strip()\n",
    "            self.reward_model_prompt = self.tokenizer.encode(\n",
    "                self.reward_model_prompt_untokenized,\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "        self.image_to_caption_mapping = None\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        logger.warning(\n",
    "            f\"Train dataset size: {len(self.train_dataset)}\",\n",
    "            # main_process_only=True\n",
    "        )  # noqa\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            collate_fn=self.data_collator,\n",
    "            batch_size=self.training_args.rollout_per_device_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        train_dataloader = self.accelerator.prepare(train_dataloader)  # noqa\n",
    "        # self._log_batch_size(train_dataloader, \"train_dataloader\")\n",
    "        return utils.InfiniteLoader(train_dataloader)\n",
    "\n",
    "    def step(self, train_dataloader, step_idx: int):\n",
    "        # TODO fix the range\n",
    "        queries_batches = [\n",
    "            next(train_dataloader) for _ in range(1)\n",
    "        ]\n",
    "        results = []\n",
    "        rollouts = self.rollout(queries_batches)\n",
    "        return rollouts\n",
    "\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def rollout(self, queries_data) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"Rollout trajectories with policy.\n",
    "\n",
    "        Args:\n",
    "            queries_data: Sequence of batches or DataLoader.\n",
    "                Each batch is a dict with keys 'queries' and 'query_attn_masks' and 'response'.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys\n",
    "                'queries', 'query_attn_masks', 'responses',\n",
    "                'logprobs', 'ref_logprobs', 'values',\n",
    "                'rewards', 'non_score_rewards', 'shaped_rewards'.\n",
    "        \"\"\"\n",
    "        # Give up dropout throughout.\n",
    "        # self.policy.eval()\n",
    "        # `keep_fp32_wrapper` retains the autocast wrapper of model.forward created by accelerate:\n",
    "        #  recall one sets mixed precision options with accelerator.\n",
    "        # The precise value of this arg doesn't matter here, since we use the unwrapped model only for respond.\n",
    "        # Generally, try to use the wrapped model as much as you can, since it's got the autocast/cast-back wrappers.\n",
    "\n",
    "\n",
    "        self.reward_model.eval()\n",
    "\n",
    "        rollouts = []\n",
    "        for batch_idx, batch in tqdm.tqdm(\n",
    "            enumerate(queries_data),\n",
    "            total=len(queries_data),\n",
    "            disable=not self.accelerator.is_main_process,\n",
    "            desc=\"rollout\",\n",
    "        ):\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # Sample rollouts.\n",
    "            (\n",
    "                images,\n",
    "                reward_images,\n",
    "                image_file_ids,\n",
    "                caption_types,\n",
    "                length_bonus_multiplier,\n",
    "                queries,\n",
    "                query_attn_masks,\n",
    "                responses,\n",
    "            ) = common_utils.unpack_dict(\n",
    "                common_utils.prepare_inputs(batch, device=self.accelerator.device),\n",
    "                keys=(\n",
    "                    \"images\",\n",
    "                    \"reward_images\",\n",
    "                    \"image_file_ids\",\n",
    "                    \"caption_types\",\n",
    "                    \"length_bonus_multiplier\",\n",
    "                    \"queries\",\n",
    "                    \"query_attn_masks\",\n",
    "                    \"responses\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            if self.args.bf16:\n",
    "                images = images.to(torch.bfloat16)\n",
    "                reward_images = reward_images.to(torch.bfloat16)\n",
    "            elif self.args.fp16:\n",
    "                images = images.half()\n",
    "                reward_images = reward_images.half()\n",
    "            # TODO: replace with the response from the dataset, not model generated\n",
    "            # respond_outputs = unwrapped_policy.respond(\n",
    "            #     queries, query_attn_masks, images, temperature=self.args.temperature\n",
    "            # )\n",
    "            # (responses,) = common_utils.unpack_dict(respond_outputs, (\"responses\",))\n",
    "\n",
    "            additional_token1 = self.tokenizer.encode(\"?\", add_special_tokens=False)[0]\n",
    "            assert additional_token1 == 1577\n",
    "\n",
    "            additional_token2 = self.tokenizer.encode(\"\\n?\")[-1]\n",
    "            assert additional_token2 == 29973\n",
    "\n",
    "            responses = truncate_after_eos_with_padding(\n",
    "                responses,\n",
    "                self.tokenizer.eos_token_id,\n",
    "                self.tokenizer.pad_token_id,\n",
    "                additional_tokens=[additional_token1, additional_token2],\n",
    "            )\n",
    "\n",
    "            rollouts_batch = {\n",
    "                \"images\": images,\n",
    "                \"reward_images\": reward_images,\n",
    "                \"queries\": queries,\n",
    "                \"query_attn_masks\": query_attn_masks,\n",
    "                \"responses\": responses,\n",
    "            }\n",
    "            # Decode the response \n",
    "            text_responses = self.tokenizer.batch_decode(\n",
    "                responses,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "            correct_bonus = []\n",
    "            # go through the text responses and assign bonus (optional)\n",
    "            # for idx, response in enumerate(text_responses):\n",
    "            #     caption_type = AnswerType(caption_types[idx].item())\n",
    "\n",
    "            #     if caption_type == AnswerType.GENERAL:\n",
    "            #         correct_bonus.append(0.0)\n",
    "            #     elif caption_type in [AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD]:\n",
    "            #         expected_start = caption_type.name.split(\"_\")[0] + \".\"\n",
    "            #         expected_phrase = \"correct option is \" + expected_start\n",
    "            #         if response.strip().startswith(expected_start) or expected_phrase in response:\n",
    "            #             correct_bonus.append(1.0)\n",
    "            #         else:\n",
    "            #             correct_bonus.append(0.0)\n",
    "            #     elif caption_type == AnswerType.NO_IN_YESNO:\n",
    "            #         if response.strip().startswith(\"No\"):\n",
    "            #             correct_bonus.append(0.5)\n",
    "            #         elif response.strip().startswith(\"Yes\"):\n",
    "            #             correct_bonus.append(-0.5)\n",
    "            #         else:\n",
    "            #             correct_bonus.append(0.0)\n",
    "            #     elif caption_type == AnswerType.YES_IN_YESNO:\n",
    "            #         # TODO(zhiqings): for now, we do not give symbolic award for \"Yes\" in Yes/No questions.\n",
    "            #         correct_bonus.append(0.0)\n",
    "            #     else:\n",
    "            #         raise NotImplementedError\n",
    "            # assert len(correct_bonus) == len(text_responses)\n",
    "            # correct_bonus = torch.tensor(correct_bonus, device=responses.device)\n",
    "\n",
    "            has_stop_token = [\n",
    "                self.tokenizer.eos_token_id in response\n",
    "                for response in responses.tolist()\n",
    "            ]\n",
    "\n",
    "            sequences = [\n",
    "                torch.concat((query, response), dim=0)\n",
    "                for query, response in zip(queries, responses)\n",
    "            ]\n",
    "            sequences = torch.stack(sequences, dim=0)\n",
    "\n",
    "            sequences = remove_pad_and_left_pad(\n",
    "                sequences,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "            # Prepareing the reward model prompt, \n",
    "            # may have fancy way to add captions or context to the prompt, dubbed as FACTUAL-RLHF in LLaVa-RLHF paper\n",
    "            if self.reward_model_prompt is not None:\n",
    "                if self.image_to_caption_mapping is None:\n",
    "                    reward_model_prompt = (\n",
    "                        self.reward_model_prompt.reshape(1, -1)\n",
    "                        .repeat(len(sequences), 1)\n",
    "                        .to(self.accelerator.device)\n",
    "                    )\n",
    "                    sequences = torch.cat((sequences, reward_model_prompt), dim=1)\n",
    "                else:\n",
    "                    reward_model_prompt_untokenized = (\n",
    "                        self.reward_model_prompt_untokenized\n",
    "                    )\n",
    "                    image_to_caption_mapping = self.image_to_caption_mapping\n",
    "\n",
    "                    image_ids = []\n",
    "                    for i in range(len(sequences)):\n",
    "                        image_file = str(image_file_ids[i].item()).zfill(12) + \".jpg\"\n",
    "                        caption_type = AnswerType(caption_types[i].item())\n",
    "                        if caption_type in [AnswerType.GENERAL, AnswerType.NO_IN_YESNO, AnswerType.YES_IN_YESNO]:\n",
    "                            image_id = image_file\n",
    "                        elif caption_type in [AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD]:\n",
    "                            image_id = \"aok_\" + image_file\n",
    "                        else:\n",
    "                            print(caption_type)\n",
    "                            print([AnswerType.GENERAL, AnswerType.NO_IN_YESNO, AnswerType.YES_IN_YESNO])\n",
    "                            print([AnswerType.A_IN_ABCD, AnswerType.B_IN_ABCD, AnswerType.C_IN_ABCD, AnswerType.D_IN_ABCD])\n",
    "                            raise NotImplementedError\n",
    "                        image_ids.append(image_id)\n",
    "\n",
    "                    captions = [\n",
    "                        image_to_caption_mapping[image_id] for image_id in image_ids\n",
    "                    ]\n",
    "\n",
    "                    assert r\"{factual_prompt}\" in reward_model_prompt_untokenized\n",
    "\n",
    "                    reward_model_prompts = []\n",
    "\n",
    "                    for caption_list in captions:\n",
    "                        caption_list = caption_list[:]\n",
    "                        random.shuffle(caption_list)\n",
    "                        factual_prompt = FACTUAL_PROMPT\n",
    "                        for caption in caption_list:\n",
    "                            factual_prompt = factual_prompt + f\"  - {caption}\\n\"\n",
    "                        reward_model_prompt_per_example = (\n",
    "                            reward_model_prompt_untokenized.format(\n",
    "                                factual_prompt=factual_prompt\n",
    "                            )\n",
    "                        )\n",
    "                        reward_model_prompts.append(reward_model_prompt_per_example)\n",
    "                    reward_model_prompts = self.tokenizer(\n",
    "                        reward_model_prompts,\n",
    "                        return_tensors=\"pt\",\n",
    "                        add_special_tokens=False,\n",
    "                        padding=\"longest\",\n",
    "                    )[\"input_ids\"]\n",
    "                    reward_model_prompts = reward_model_prompts.to(\n",
    "                        self.accelerator.device\n",
    "                    )\n",
    "\n",
    "                    sequences = torch.cat((sequences, reward_model_prompts), dim=1)\n",
    "                    sequences = remove_pad_and_left_pad(\n",
    "                        sequences,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    )\n",
    "\n",
    "            clean_sequences = sequences.detach().clone()\n",
    "            clean_sequences[\n",
    "                clean_sequences == IMAGE_TOKEN_INDEX\n",
    "            ] = self.tokenizer.eos_token_id\n",
    "\n",
    "            text_sequences = self.tokenizer.batch_decode(\n",
    "                clean_sequences,\n",
    "                skip_special_tokens=False,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "            if self.accelerator.is_main_process:\n",
    "                print(\"=\" * 20)\n",
    "                print(text_sequences[0].split(\"<unk><s> \")[-1])\n",
    "                print(\"=\" * 20)\n",
    "                image_id = image_file_ids[0].item()\n",
    "                # convert int into \"000000xxxxxx.jpg\"\n",
    "                image_id = (\n",
    "                    \"https://s3.us-east-1.amazonaws.com/images.cocodataset.org/train2017/\"\n",
    "                    + str(image_id).zfill(12)\n",
    "                    + \".jpg\"\n",
    "                )\n",
    "                print(image_id)\n",
    "                print(\"=\" * 20)\n",
    "            # OPTIONAL: compute the length bonus \n",
    "            non_pad_mask = responses.ne(self.tokenizer.pad_token_id)\n",
    "            non_pad_seq_len = (\n",
    "                non_pad_mask.sum(dim=1).float().to(self.accelerator.device)\n",
    "            )\n",
    "            length_bonus = non_pad_seq_len / float(self.args.response_len)\n",
    "\n",
    "            # convert length_bonus_multiplier to the shape, type, and device of length_bonus\n",
    "            length_bonus = length_bonus * length_bonus_multiplier.to(\n",
    "                length_bonus.device\n",
    "            ).reshape(length_bonus.shape).to(length_bonus.dtype)\n",
    "\n",
    "            sequences_attention_mask = sequences.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "            # Evaluate logprobs of the samples.\n",
    "\n",
    "\n",
    "\n",
    "            rollouts_batch[\"length_bonus\"] = length_bonus\n",
    "            rollouts_batch[\"correct_bonus\"] = correct_bonus\n",
    "            sub_batch_size = self.args.reward_model_per_device_batch_size\n",
    "            batch_size_per_device = rollouts_batch[\"responses\"].shape[0]\n",
    "            if sub_batch_size is None or sub_batch_size == batch_size_per_device:\n",
    "                reward_outputs = self.reward_model(\n",
    "                    input_ids=sequences,\n",
    "                    attention_mask=sequences_attention_mask,\n",
    "                    images=reward_images,\n",
    "                )\n",
    "                print(reward_outputs)\n",
    "            else:\n",
    "                assert batch_size_per_device % sub_batch_size == 0\n",
    "\n",
    "                reward_outputs_list = []\n",
    "\n",
    "                for sub_batch_idx in range(batch_size_per_device // sub_batch_size):\n",
    "                    idx_start = sub_batch_idx * sub_batch_size\n",
    "                    idx_end = (sub_batch_idx + 1) * sub_batch_size\n",
    "                    sub_batch_reward_outputs = self.reward_model(\n",
    "                        input_ids=sequences[idx_start:idx_end],\n",
    "                        attention_mask=sequences_attention_mask[idx_start:idx_end],\n",
    "                        images=reward_images[idx_start:idx_end],\n",
    "                    )\n",
    "                    reward_outputs_list.append(sub_batch_reward_outputs)\n",
    "\n",
    "                reward_outputs = common_utils.merge_dict(\n",
    "                    reward_outputs_list, merge_fn=torch.cat\n",
    "                )\n",
    "                del reward_outputs_list\n",
    "                del sub_batch_reward_outputs\n",
    "            # Remove the penality for sequences that did not stop properly\n",
    "            # reward_outputs = self.post_reward(\n",
    "            #     reward_outputs,\n",
    "            #     responses,\n",
    "            #     penalize_no_stop_token=self.args.penalize_no_stop_token,\n",
    "            #     relative_stop_token_penalty=self.args.relative_stop_token_penalty,\n",
    "            #     has_stop_token=has_stop_token,\n",
    "            # )\n",
    "            rollouts_batch.update(reward_outputs)\n",
    "            print(f'rollouts_batch: {rollouts_batch}')\n",
    "\n",
    "            # Shape reward with KL penalty.\n",
    "            # shape_reward_outputs = self._shape_reward(\n",
    "            #     rewards=rollouts_batch[\"rewards\"],\n",
    "            #     responses=rollouts_batch[\"responses\"],\n",
    "            #     logprobs=rollouts_batch[\"logprobs\"],\n",
    "            #     ref_logprobs=rollouts_batch[\"ref_logprobs\"],\n",
    "            #     length_bonus=rollouts_batch[\"length_bonus\"],\n",
    "            #     correct_bonus=rollouts_batch[\"correct_bonus\"],\n",
    "            # )\n",
    "            rollouts_batch_cpu = {\n",
    "                key: value for key, value in rollouts_batch.items()\n",
    "            }\n",
    "            rollouts.append(rollouts_batch_cpu)\n",
    "\n",
    "        # # Items in dict need to be of same shape.\n",
    "        # rollouts = common_utils.merge_dict(rollouts, merge_fn=torch.cat)\n",
    "\n",
    "        # # Estimating advantages outside the loop gives more samples for reward normalization.\n",
    "        # advantages = self._estimate_advantage(\n",
    "        #     rewards=rollouts[\"shaped_rewards\"].to(self.accelerator.device),\n",
    "        #     values=rollouts[\"values\"].to(self.accelerator.device),\n",
    "        # )\n",
    "        # advantages = {key: value.cpu() for key, value in advantages.items()}\n",
    "        # pdb.set_trace()\n",
    "        return rollouts_batch\n",
    "\n",
    "\n",
    "    def post_reward(\n",
    "        self,\n",
    "        reward_outputs: Dict[str, torch.Tensor],\n",
    "        responses: torch.Tensor,\n",
    "        penalize_no_stop_token: bool,\n",
    "        relative_stop_token_penalty: bool,\n",
    "        has_stop_token: List[bool],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Assign bad reward values to sequences which didn't stop properly.\"\"\"\n",
    "        if penalize_no_stop_token:\n",
    "            has_stop_token = torch.tensor(has_stop_token, device=responses.device)\n",
    "            rewards = reward_outputs[\"rewards\"]\n",
    "            if relative_stop_token_penalty:\n",
    "                rewards = (\n",
    "                    rewards + (~has_stop_token).float() * self.args.penalty_reward_value\n",
    "                )\n",
    "            else:\n",
    "                rewards[~has_stop_token] = self.args.penalty_reward_value\n",
    "            reward_outputs[\"rewards\"] = rewards\n",
    "            return reward_outputs\n",
    "\n",
    "        if self.args.truncate_token_ids is None:\n",
    "            return reward_outputs\n",
    "\n",
    "accelerator = AlpacaAccelerator(\n",
    "    log_with=training_args.report_to,\n",
    "    project_dir=training_args.logging_dir,\n",
    "    gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "    even_batches=True,  # Make sure the batch size on each device is the same.\n",
    "    split_batches=False,  # Don't break a batch into smaller chunks.\n",
    "    step_scheduler_with_optimizer=False,  # Untie optimizer and scheduler step.\n",
    "    # Value model might not use all parameters (e.g., lm-head) in the forward pass.\n",
    "    kwargs_handlers=[\n",
    "        DistributedDataParallelKwargs(\n",
    "            find_unused_parameters=training_args.ddp_find_unused_parameters,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "attribute_model = GetAttributeModel(rm_model, tokenizer, accelerator, training_args)\n",
    "attribute_model.training_args.rollout_per_device_batch_size = 4\n",
    "train_dataloader = attribute_model.get_train_dataloader()\n",
    "for i in range(1):\n",
    "    rollout = attribute_model.step(train_dataloader, i)\n",
    "    print(rollout.keys())\n",
    "    # Saves the attributes to the prompt\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bazure/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/load_sft_and_rm_adapter.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49msoftmax(rollout[\u001b[39m'\u001b[39;49m\u001b[39mrewards\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (Tensor), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype, *, Tensor out)\n * (Tensor input, name dim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "torch.softmax(rollout['rewards'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llavav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
