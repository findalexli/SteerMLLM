{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import traceback\n",
    "import ast\n",
    "# This file comes right after using gpt 3.5 turbo to extract the attrbutes from the origional string\n",
    "with open('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/extracted_attributes_1225_1425.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Ratings for Response A\": {\\n    \"Hallucinations\": 0.3,\\n    \"Helpfulness\": 0.7,\\n    \"Quality\": 0.6,\\n    \"Spatial-Awareness\": 0.7,\\n    \"Domain-Knowledge\": 0.5\\n  },\\n  \"Ratings for Response B\": {\\n    \"Hallucinations\": 0.7,\\n    \"Helpfulness\": 0.6,\\n    \"Quality\": 0.5,\\n    \"Spatial-Awareness\": 0.5,\\n    \"Domain-Knowledge\": 0.5\\n  }\\n}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['extracted_attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Ratings for Response A\": {\\n    \"Hallucinations\": 0.1,\\n    \"Helpfulness\": 0.8,\\n    \"Quality\": 0.8,\\n    \"Spatial-Awareness\": 1.0,\\n    \"Domain-Knowledge\": 0.8\\n  },\\n  \"Ratings for Response B\": {\\n    \"Hallucinations\": 0.1,\\n    \"Helpfulness\": 0.8,\\n    \"Quality\": 0.8,\\n    \"Spatial-Awareness\": 1.0,\\n    \"Domain-Knowledge\": 0.7\\n  }\\n}'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/temp_30_1224.csv')\n",
    "# check if two df have the same columns\n",
    "df2.iloc[0]['extracted_attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/extracted_attributes_1525_2125.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "df3 = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Paths to the JSON files\n",
    "# file1 = 'extracted_attributes_sft_0_1224-likert_scale1118.json'\n",
    "# file2 = 'extracted_attributes_sft_1225-1525-likert_scale1118.json'\n",
    "# file3 = 'extracted_attributes_sft_30_1525-2125-likert_scale1118.json'\n",
    "\n",
    "# # Load each file into a DataFrame\n",
    "# df1 = pd.read_json(file1, orient='records')\n",
    "# df2 = pd.read_json(file2, orient='records')\n",
    "# df3 = pd.read_json(file3, orient='records')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df, df2, df3])\n",
    "\n",
    "# Save the combined DataFrame as a new JSON file\n",
    "# combined_df.to_json('combined.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trim_conversation(conversation):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(conversation, str):\n",
    "        conversation_list = ast.literal_eval(conversation)\n",
    "    elif isinstance(conversation, list):\n",
    "        conversation_list = conversation\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    # Keep only the last two turns of the conversation (one human and one gpt turn)\n",
    "    if len(conversation_list) >= 2:\n",
    "        trimmed_conversation = conversation_list[-2:]\n",
    "    else:\n",
    "        raise Exception(\"The conversation is too short to trim.\")\n",
    "    # return only the human turn\n",
    "    trimmed_conversation[-2]['value'] = '<image>\\n' + trimmed_conversation[-2]['value']\n",
    "    return trimmed_conversation[-2]['value']\n",
    "\n",
    "def _get_output(conversation):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(conversation, str):\n",
    "        conversation_list = ast.literal_eval(conversation)\n",
    "    elif isinstance(conversation, dict):\n",
    "        conversation_list = conversation\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    # Keep only the last two turns of the conversation (one human and one gpt turn)\n",
    "\n",
    "    # return only the human turn\n",
    "    output = conversation_list['value']\n",
    "    return output\n",
    "\n",
    "def _get_gpt4_response(gpt4v_response):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(gpt4v_response, str):\n",
    "        gpt4v_response = ast.literal_eval(gpt4v_response)\n",
    "    elif isinstance(gpt4v_response, dict):\n",
    "        gpt4v_response = gpt4v_response\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    \n",
    "    return gpt4v_response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def extract_data(df):\n",
    "    extracted_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract the desired information from each row\n",
    "            human_turn = _trim_conversation(row['conversations'])\n",
    "            output_1 = _get_output(row['output_1'])\n",
    "            output_2 = _get_output(row['output_2'])\n",
    "            gpt_response_text = _get_gpt4_response(row['gpt4v_response'])\n",
    "        except Exception as e:\n",
    "            # Handle the case when an exception occurs\n",
    "            print(f\"Error occurred for row {index}: {e}\")\n",
    "            human_turn = \"\"\n",
    "            output_1 = \"\"\n",
    "            output_2 = \"\"\n",
    "            gpt_response_text = \"\"\n",
    "        \n",
    "        # Append the extracted data to the list\n",
    "        extracted_data.append((human_turn, output_1, output_2, gpt_response_text))\n",
    "    \n",
    "    # Create new columns for the extracted data\n",
    "    df['question_string'] = [data[0] for data in extracted_data]\n",
    "    df['Output_1_string'] = [data[1] for data in extracted_data]\n",
    "    df['Output_2_string'] = [data[2] for data in extracted_data]\n",
    "    df['GPT_teacher_critiq_string'] = [data[3] for data in extracted_data]\n",
    "    \n",
    "    return df\n",
    "\n",
    "combined_extract = extract_data(combined_df)\n",
    "combined_extract['question_id'] = combined_extract.apply(lambda row: row['id'], axis=1)\n",
    "combined_extract.to_json('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/gpt4_ratings/training_data/teacher-critique-0-2125.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response A and B are both based on the same image which depicts a cow in a field, separated from the viewer by a barbed wire fence. Both responses assume the cow\\'s living conditions to be good based on its environment, which is a spacious area with grass coverage.\\n\\nResponse A goes into more detail about the conditions necessary for cow welfare, suggesting a holistic approach to understanding the image. It correctly identifies a single cow and refers to the fence that delineates the cow\\'s grazing area. Additionally, it gives an extensive interpretation of how the setting might affect the well-being of the cow.\\n\\nResponse B similarly identifies the cow and its environment, mentioning the barbed wire fence and implying that the cow is well-taken care of due to its environment. It portrays the cow as healthy-looking and states the natural habitat is suitable for its needs.\\n\\nHere are the ratings for each response:\\n\\nRatings for Response A:\\n```json\\n{\\n  \"Hallucinations\": 0.1, // The response slightly embellishes on the well-being of the cow without having complete information, but there\\'s no direct hallucination.\\n  \"Helpfulness\": 0.8, // It provides a well-rounded perspective on the cow\\'s potential living conditions.\\n  \"Quality\": 0.8, // The response is well-articulated and relevant to the image.\\n  \"Spatial-Awareness\": 1.0, // Accurately describes the spatial elements: the cow, the field, and the fence.\\n  \"Domain Knowledge\": 0.8 // Shows good knowledge of livestock management and animal welfare.\\n}\\n```\\n\\nRatings for Response B:\\n```json\\n{\\n  \"Hallucinations\": 0.1, // Similar to Response A, there are assumptions about the cow\\'s well-being but no clear inaccuracies.\\n  \"Helpfulness\": 0.8, // It addresses the cow\\'s environment and provides an assessment of its living conditions.\\n  \"Quality\": 0.8, // The response is coherent and provides important details about the image.\\n  \"Spatial-Awareness\": 1.0, // It correctly identifies and describes the spatial elements present in the image.\\n  \"Domain Knowledge\": 0.7 // Displays knowledge of animal care but is not as detailed as Response A.\\n}\\n```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_extract['GPT_teacher_critiq_string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last question turn as prompt from 'conversations'\n",
    "human_turn = _trim_conversation(df2.iloc[0]['conversations'])\n",
    "# get the first output_1 \n",
    "output_1 = _get_output(df2.iloc[0]['output_1'])\n",
    "# get the second output_2\n",
    "output_2 = _get_output(df2.iloc[0]['output_2'])\n",
    "# provide the gpt4v_response\n",
    "gpt_repsonse_text = _get_gpt4_response(df2.iloc[0]['gpt4v_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1151"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava1dot5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
