{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:  42 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3009_3109.json after filtering:  42\n",
      "Ratio of rows kept:  1.0\n",
      "Total number of rows:  2 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3109_3150.json after filtering:  2\n",
      "Ratio of rows kept:  1.0\n",
      "Total number of rows:  75 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3050_3150.json after filtering:  75\n",
      "Ratio of rows kept:  1.0\n",
      "Total number of rows:  75 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3150_3250.json after filtering:  75\n",
      "Ratio of rows kept:  1.0\n",
      "Total number of rows:  100 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3250_3350.json after filtering:  100\n",
      "Ratio of rows kept:  1.0\n",
      "Total number of rows:  700 for file:  llava_7b_v1_preference_subsample_gpt4v_response_3009_3909_combined.json after filtering:  118\n",
      "Ratio of rows kept:  0.16857142857142857\n",
      "Total number of rows:  700 for file:  llava_7b_v1_preference_subsample_gpt4v_response_2308_3208_combined.json after filtering:  629\n",
      "Ratio of rows kept:  0.8985714285714286\n"
     ]
    }
   ],
   "source": [
    "# load the new json file\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "file_list = ['/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3009_3109.json', \n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3109_3150.json',\n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3050_3150.json', \n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3150_3250.json', \n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3250_3350.json', \n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_3009_3909_combined.json', \n",
    "             '/home/ubuntu/RLHF/LLaVA-RLHF-Data/llava_7b_v1_preference_subsample_gpt4v_response_2308_3208_combined.json',]\n",
    "def _check_row(row):\n",
    "    gpt_response = row['gpt4v_response']\n",
    "    if isinstance(gpt_response, str):\n",
    "        gpt_response = json.loads(gpt_response)\n",
    "    assert isinstance(gpt_response, dict), \"gpt_response is not a dict\"\n",
    "    if \"error\" in gpt_response:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def load_only_rows_wo_error(file):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        filtered_data = {key: row for key, row in data.items() if _check_row(row)}\n",
    "        print(\"Total number of rows: \", len(data), 'for file: ', file.rsplit('/')[-1], 'after filtering: ', len(filtered_data))\n",
    "        print(\"Ratio of rows kept: \", len(filtered_data)/len(data))\n",
    "    return filtered_data\n",
    "\n",
    "def load_all_files(file_list):\n",
    "    data = {}\n",
    "    for file in file_list:\n",
    "        data.update(load_only_rows_wo_error(file))\n",
    "    return data\n",
    "\n",
    "data = load_all_files(file_list)\n",
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('unextracted_llava_7b_v1_preference_subsample_gpt4v_response_2308_3909_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'my_response\\': \\'The image shows a single airplane on the tarmac with the markings \"Armavia\" on its fuselage, suggesting it belongs to the now-defunct Armenian airline. There are no multiple airplanes visible in the picture. The aircraft appears to be an Airbus A319 or similar model based on the design features seen, such as the size, shape, and number of windows. There are service vehicles around the aircraft, including what appears to be a baggage conveyor and another utility vehicle, implying it is being serviced, perhaps for an upcoming flight or after landing. Despite the lack of multiple airplanes, the servicing activity suggests efficient ground operations, necessary for quick turnaround times and maintaining flight schedules.\\',\\n\\'Ratings\\': {\\'Ratings4CandidateResponseA\\': {\\'CommentSection\\': \\'Response A incorrectly assumes multiple airplanes are present and elaborates on implications that are not relevant to the actual image. The description of large airplanes and implications for international flights, while potentially accurate in a different context, does not apply here.\\', \\'Hallucinations\\': 1.0, \\'Helpfulness\\': 0.1, \\'Quality\\': 0.1, \\'Spatial-Awareness\\': 0.0, \\'Domain-Knowledge\\': 0.1},\\n\\'Ratings4CandidateResponseB\\': {\\'CommentSection\\': \\'Response B makes a similar error as Response A, discussing multiple airplanes when only one is present. Though the general implications it discusses could be accurate in a different situation, it is not relevant to this image.\\', \\'Hallucinations\\': 1.0, \\'Helpfulness\\': 0.1, \\'Quality\\': 0.1, \\'Spatial-Awareness\\': 0.0, \\'Domain-Knowledge\\': 0.1},\\n\\'Ratings4CandidateResponseC\\': {\\'CommentSection\\': \\'Response C also hallucinates the presence of multiple airplanes when only one airplane is visible, making the implications mentioned irrelevant. The response does identify the need for efficient management and customer experience, which are correct in general airport scenarios, but it still fails to address what is actually depicted.\\', \\'Hallucinations\\': 1.0, \\'Helpfulness\\': 0.1, \\'Quality\\': 0.1, \\'Spatial-Awareness\\': 0.0, \\'Domain-Knowledge\\': 0.1},\\n\\'Ratings4YourOwnResponseYouWrote\\': {\\'CommentSection\\': \\'My response avoids the hallucination present in the other candidates by accurately describing the single airplane that is visible in the image. It also correctly identifies the type of aircraft and the operational activities taking place, such as servicing, without making incorrect assumptions about the number of airplanes or the implications of such a scene.\\', \\'Hallucinations\\': 0.0, \\'Helpfulness\\': 0.9, \\'Quality\\': 0.9, \\'Spatial-Awareness\\': 1.0, \\'Domain-Knowledge\\': 0.8}}}'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['gpt4v_response']['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"chatcmpl-8OA1thObFMmJj1omYPKnUuD9JDyjA\", \"choices\": [{\"finish_reason\": null, \"index\": 0, \"message\": {\"content\": \"{\\'my_response\\': \\'The key elements in this image include a group of approximately seventeen men gathered outside a decorated train car. The men are clad predominantly in hats and suits, indicating formal wear reflective of an earlier time period. Some men are seated on the ground, others are standing, and two men are positioned on the steps of the train car, suggesting a posed group photo. The train car itself features elaborate artwork with the words \\\\\"Barnum & Bailey\\\\\" and \\\\\"Greatest Show on Earth,\\\\\" indicating that this is likely associated with the historic Barnum & Bailey Circus. The photograph is monochromatic, possibly dating back to the late 19th or early 20th century. Moreover, the image includes details such as the wooden planks and railroad tracks on which the train car is situated.\\',\\\\n\\'Ratings\\': {\\'Ratings4CandidateResponseA\\': {\\'CommentSection\\': \\'Candidate A provided a detailed description of the image that aligns with its content. However, they have inaccurately stated the number of men as \\\\\"at least thirteen\\\\\" when there is a total of seventeen. This counts as a minor hallucination. The response captures the formal attire of the individuals and the placement of the train though it doesn\\'t mention the significant text on the train car which provides important context.\\', \\'Hallucinations\\': 0.1, \\'Helpfulness\\': 0.8, \\'Quality\\': 0.8, \\'Spatial-Awareness\\': 0.9, \\'Domain-Knowledge\\': 0.8},\\\\n\\'Ratings4CandidateResponseB\\': {\\'CommentSection\\': \\'Candidate B accurately describes key elements but fails to notice and thus mention the specifics like the number of people and the text on the train car \\\\\"Barnum & Bailey\\\\\" which is crucial for contextual understanding. Moreover, stating they appear to be in their \\\\\"Sunday best\\\\\" is an assumption and thus introduces a slight hallucination as we cannot confirm the occasion.\\', \\'Hallucinations\\': 0.2, \\'Helpfulness\\': 0.7, \\'Quality\\': 0.7, \\'Spatial-Awareness\\': 0.8, \\'Domain-Knowledge\\': 0.6},\\\\n\\'Ratings4CandidateResponseC\\': {\\'CommentSection\\': \\'Candidate C provides a succinct and accurate description of the image, mentioning the formal attire and the train car. Yet, they did not specify the number of people and omitted mentioning the circus-related text on the train car which is a significant contextual detail. Omitting such an important detail reduces the helpfulness and quality of the response.\\', \\'Hallucinations\\': 0.0, \\'Helpfulness\\': 0.75, \\'Quality\\': 0.75, \\'Spatial-Awareness\\': 0.85, \\'Domain-Knowledge\\': 0.7},\\\\n\\'Ratings4YourOwnResponseYouWrote\\': {\\'CommentSection\\': \\'The response accurately identifies the group of seventeen men and mentions their formal attire. It also correctly names the circus and references the historical time period. The description includes the train, the text on the train car, and the setting. There are no hallucinations, and the response is coherent, effectively addresses the user\\\\u2019s query, and demonstrates spatial awareness and domain knowledge.\\', \\'Hallucinations\\': 0.0, \\'Helpfulness\\': 1.0, \\'Quality\\': 1.0, \\'Spatial-Awareness\\': 1.0, \\'Domain-Knowledge\\': 1.0}}}\\\\n\", \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}, \"finish_details\": {\"type\": \"stop\", \"stop\": \"<|fim_suffix|>\"}}], \"created\": 1700769117, \"model\": \"gpt-4-1106-vision-preview\", \"object\": \"chat.completion\", \"system_fingerprint\": null, \"usage\": {\"completion_tokens\": 700, \"prompt_tokens\": 1997, \"total_tokens\": 2697}}'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[19]['gpt4v_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "import traceback\n",
    "for i in range(len(df)):\n",
    "    try: \n",
    "        response_body = df.iloc[i]['gpt4v_response']\n",
    "        if isinstance(response_body, str):\n",
    "            response_body = json.loads(response_body)\n",
    "        response_string = response_body['choices'][0]['message']['content']\n",
    "        df.iloc[0]['response_string'] = response_string\n",
    "    except:\n",
    "        print(df.iloc[i]['gpt4v_response'])\n",
    "        count += 1\n",
    "        print(i)\n",
    "        traceback.print_exc()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'my_response': 'The black ceramic bowl with the small blue and black bird inside is placed on a wooden surface, and there is sunlight shining on the scene, likely coming from a window to the left.',\\n'Ratings': {'Ratings4CandidateResponseA': {'CommentSection': 'Response A correctly identifies that the bowl is placed on a table, which is somewhat visible in the image. However, it doesn't mention the wooden material of the table or the presence of sunlight, which are both important details in the setting.', 'Hallucinations': 0.0, 'Helpfulness': 0.5, 'Quality': 0.5, 'Spatial-Awareness': 0.5, 'Domain-Knowledge': 0.5},\\n'Ratings4CandidateResponseB': {'CommentSection': 'Response B expands on Response A by correctly noting that the setting is sunny. However, it hallucinates the bowl sitting in the sun, which might imply it is outdoors, while the image doesn't provide enough context to confirm that. It also does not mention the wooden material of the table.', 'Hallucinations': 0.5, 'Helpfulness': 0.6, 'Quality': 0.5, 'Spatial-Awareness': 0.5, 'Domain-Knowledge': 0.5},\\n'Ratings4CandidateResponseC': {'CommentSection': 'Response C provides the most detailed description by mentioning the colors of the bird and bowl, as well as identifying the table as wooden. However, it fails to include the presence of sunlight, which is a prominent feature of the setting.', 'Hallucinations': 0.0, 'Helpfulness': 0.7, 'Quality': 0.7, 'Spatial-Awareness': 0.7, 'Domain-Knowledge': 0.7},\\n'Ratings4YourOwnResponseYouWrote': {'CommentSection': 'My response provides a comprehensive description including the color of the bowl and bird, the material of the table, and the presence of sunlight, which accurately reflects the setting shown in the image.', 'Hallucinations': 0.0, 'Helpfulness': 0.9, 'Quality': 0.9, 'Spatial-Awareness': 0.9, 'Domain-Knowledge': 0.8},\\n}}\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (<unknown>, line 2)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3548\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[34], line 2\u001b[0m\n    ast.literal_eval(response_string)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:2\u001b[0;36m\u001b[0m\n\u001b[0;31m    'Ratings': {'Ratings4CandidateResponseA': {'CommentSection': 'Response A correctly identifies that the bowl is placed on a table, which is somewhat visible in the image. However, it doesn't mention the wooden material of the table or the presence of sunlight, which are both important details in the setting.', 'Hallucinations': 0.0, 'Helpfulness': 0.5, 'Quality': 0.5, 'Spatial-Awareness': 0.5, 'Domain-Knowledge': 0.5},\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "response_string = df.iloc[10]['gpt4v_response']['choices'][0]['message']['content']\n",
    "ast.literal_eval(response_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (<unknown>, line 2)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3548\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[31], line 11\u001b[0m\n    num_rows_extracted = df.apply(_extract_response_into_dict, axis=1).count()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/pandas/core/frame.py:10037\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return op.apply().__finalize__(self, method=\"apply\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/pandas/core/apply.py:837\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/pandas/core/apply.py:963\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    results, res_index = self.apply_series_generator()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/site-packages/pandas/core/apply.py:979\u001b[0m in \u001b[1;35mapply_series_generator\u001b[0m\n    results[i] = self.func(v, *self.args, **self.kwargs)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[31], line 8\u001b[0m in \u001b[1;35m_extract_response_into_dict\u001b[0m\n    response_dict = ast.literal_eval(response_string)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/mambaforge-pypy3/envs/llava1dot5/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:2\u001b[0;36m\u001b[0m\n\u001b[0;31m    'Ratings': {'Ratings4CandidateResponseA': {'CommentSection': 'Response A incorrectly states that the bicycle is parked on a platform near a train, which could imply that the train is very close, while in the image, the train is in the background. It also falsely claims that the bicycle is resting on a bench, and mentions people scattered which are not visible in the image at all. Lastly, it falsely identifies two backpacks which don't exist in the scene.', 'Hallucinations': 0.9, 'Helpfulness': 0.3, 'Quality': 0.3, 'Spatial-Awareness': 0.3, 'Domain-Knowledge': 0.3},\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "def _extract_response_into_dict(row):\n",
    "    gpt_response = row['gpt4v_response']\n",
    "    if isinstance(gpt_response, str):\n",
    "        gpt_response = json.loads(gpt_response) \n",
    "    assert isinstance(gpt_response, dict), \"gpt_response is not a dict\"\n",
    "    response_string = gpt_response['choices'][0]['message']['content']\n",
    "    response_dict = ast.literal_eval(response_string)\n",
    "    return response_dict\n",
    "\n",
    "num_rows_extracted = df.apply(_extract_response_into_dict, axis=1).count()\n",
    "print(\"Number of rows that can be extracted directly:\", num_rows_extracted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import traceback\n",
    "import ast\n",
    "# This file comes right after using gpt 3.5 turbo to extract the attrbutes from the origional string\n",
    "with open('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/extracted_attributes_1225_1425.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "df = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Ratings for Response A\": {\\n    \"Hallucinations\": 0.3,\\n    \"Helpfulness\": 0.7,\\n    \"Quality\": 0.6,\\n    \"Spatial-Awareness\": 0.7,\\n    \"Domain-Knowledge\": 0.5\\n  },\\n  \"Ratings for Response B\": {\\n    \"Hallucinations\": 0.7,\\n    \"Helpfulness\": 0.6,\\n    \"Quality\": 0.5,\\n    \"Spatial-Awareness\": 0.5,\\n    \"Domain-Knowledge\": 0.5\\n  }\\n}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['extracted_attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"Ratings for Response A\": {\\n    \"Hallucinations\": 0.1,\\n    \"Helpfulness\": 0.8,\\n    \"Quality\": 0.8,\\n    \"Spatial-Awareness\": 1.0,\\n    \"Domain-Knowledge\": 0.8\\n  },\\n  \"Ratings for Response B\": {\\n    \"Hallucinations\": 0.1,\\n    \"Helpfulness\": 0.8,\\n    \"Quality\": 0.8,\\n    \"Spatial-Awareness\": 1.0,\\n    \"Domain-Knowledge\": 0.7\\n  }\\n}'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/temp_30_1224.csv')\n",
    "# check if two df have the same columns\n",
    "df2.iloc[0]['extracted_attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/extracted_attributes_1525_2125.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "df3 = pd.DataFrame.from_dict(data, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Paths to the JSON files\n",
    "# file1 = 'extracted_attributes_sft_0_1224-likert_scale1118.json'\n",
    "# file2 = 'extracted_attributes_sft_1225-1525-likert_scale1118.json'\n",
    "# file3 = 'extracted_attributes_sft_30_1525-2125-likert_scale1118.json'\n",
    "\n",
    "# # Load each file into a DataFrame\n",
    "# df1 = pd.read_json(file1, orient='records')\n",
    "# df2 = pd.read_json(file2, orient='records')\n",
    "# df3 = pd.read_json(file3, orient='records')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df, df2, df3])\n",
    "\n",
    "# Save the combined DataFrame as a new JSON file\n",
    "# combined_df.to_json('combined.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _trim_conversation(conversation):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(conversation, str):\n",
    "        conversation_list = ast.literal_eval(conversation)\n",
    "    elif isinstance(conversation, list):\n",
    "        conversation_list = conversation\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    # Keep only the last two turns of the conversation (one human and one gpt turn)\n",
    "    if len(conversation_list) >= 2:\n",
    "        trimmed_conversation = conversation_list[-2:]\n",
    "    else:\n",
    "        raise Exception(\"The conversation is too short to trim.\")\n",
    "    # return only the human turn\n",
    "    trimmed_conversation[-2]['value'] = '<image>\\n' + trimmed_conversation[-2]['value']\n",
    "    return trimmed_conversation[-2]['value']\n",
    "\n",
    "def _get_output(conversation):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(conversation, str):\n",
    "        conversation_list = ast.literal_eval(conversation)\n",
    "    elif isinstance(conversation, dict):\n",
    "        conversation_list = conversation\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    # Keep only the last two turns of the conversation (one human and one gpt turn)\n",
    "\n",
    "    # return only the human turn\n",
    "    output = conversation_list['value']\n",
    "    return output\n",
    "\n",
    "def _get_gpt4_response(gpt4v_response):\n",
    "    # Parse the conversation string into a list of dictionaries\n",
    "    if isinstance(gpt4v_response, str):\n",
    "        gpt4v_response = ast.literal_eval(gpt4v_response)\n",
    "    elif isinstance(gpt4v_response, dict):\n",
    "        gpt4v_response = gpt4v_response\n",
    "    else:\n",
    "        raise Exception(\"The conversation is not in the correct format.\")\n",
    "    \n",
    "    return gpt4v_response['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def extract_data(df):\n",
    "    extracted_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            # Extract the desired information from each row\n",
    "            human_turn = _trim_conversation(row['conversations'])\n",
    "            output_1 = _get_output(row['output_1'])\n",
    "            output_2 = _get_output(row['output_2'])\n",
    "            gpt_response_text = _get_gpt4_response(row['gpt4v_response'])\n",
    "        except Exception as e:\n",
    "            # Handle the case when an exception occurs\n",
    "            print(f\"Error occurred for row {index}: {e}\")\n",
    "            human_turn = \"\"\n",
    "            output_1 = \"\"\n",
    "            output_2 = \"\"\n",
    "            gpt_response_text = \"\"\n",
    "        \n",
    "        # Append the extracted data to the list\n",
    "        extracted_data.append((human_turn, output_1, output_2, gpt_response_text))\n",
    "    \n",
    "    # Create new columns for the extracted data\n",
    "    df['question_string'] = [data[0] for data in extracted_data]\n",
    "    df['Output_1_string'] = [data[1] for data in extracted_data]\n",
    "    df['Output_2_string'] = [data[2] for data in extracted_data]\n",
    "    df['GPT_teacher_critiq_string'] = [data[3] for data in extracted_data]\n",
    "    \n",
    "    return df\n",
    "\n",
    "combined_extract = extract_data(combined_df)\n",
    "combined_extract['question_id'] = combined_extract.apply(lambda row: row['id'], axis=1)\n",
    "combined_extract.to_json('/home/ubuntu/RLHF/LLaVA-RLHF/RLHF/gpt4_ratings/training_data/teacher-critique-0-2125.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response A and B are both based on the same image which depicts a cow in a field, separated from the viewer by a barbed wire fence. Both responses assume the cow\\'s living conditions to be good based on its environment, which is a spacious area with grass coverage.\\n\\nResponse A goes into more detail about the conditions necessary for cow welfare, suggesting a holistic approach to understanding the image. It correctly identifies a single cow and refers to the fence that delineates the cow\\'s grazing area. Additionally, it gives an extensive interpretation of how the setting might affect the well-being of the cow.\\n\\nResponse B similarly identifies the cow and its environment, mentioning the barbed wire fence and implying that the cow is well-taken care of due to its environment. It portrays the cow as healthy-looking and states the natural habitat is suitable for its needs.\\n\\nHere are the ratings for each response:\\n\\nRatings for Response A:\\n```json\\n{\\n  \"Hallucinations\": 0.1, // The response slightly embellishes on the well-being of the cow without having complete information, but there\\'s no direct hallucination.\\n  \"Helpfulness\": 0.8, // It provides a well-rounded perspective on the cow\\'s potential living conditions.\\n  \"Quality\": 0.8, // The response is well-articulated and relevant to the image.\\n  \"Spatial-Awareness\": 1.0, // Accurately describes the spatial elements: the cow, the field, and the fence.\\n  \"Domain Knowledge\": 0.8 // Shows good knowledge of livestock management and animal welfare.\\n}\\n```\\n\\nRatings for Response B:\\n```json\\n{\\n  \"Hallucinations\": 0.1, // Similar to Response A, there are assumptions about the cow\\'s well-being but no clear inaccuracies.\\n  \"Helpfulness\": 0.8, // It addresses the cow\\'s environment and provides an assessment of its living conditions.\\n  \"Quality\": 0.8, // The response is coherent and provides important details about the image.\\n  \"Spatial-Awareness\": 1.0, // It correctly identifies and describes the spatial elements present in the image.\\n  \"Domain Knowledge\": 0.7 // Displays knowledge of animal care but is not as detailed as Response A.\\n}\\n```'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_extract['GPT_teacher_critiq_string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last question turn as prompt from 'conversations'\n",
    "human_turn = _trim_conversation(df2.iloc[0]['conversations'])\n",
    "# get the first output_1 \n",
    "output_1 = _get_output(df2.iloc[0]['output_1'])\n",
    "# get the second output_2\n",
    "output_2 = _get_output(df2.iloc[0]['output_2'])\n",
    "# provide the gpt4v_response\n",
    "gpt_repsonse_text = _get_gpt4_response(df2.iloc[0]['gpt4v_response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1151"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava1dot5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
